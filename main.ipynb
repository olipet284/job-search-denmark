{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce7c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import linkedin_scraper, jobnet_scraper, jobindex_scraper\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Data Scientist\" # Notice, the search function within each Job Board will allow to find similar positions, e.g. \"Data Analyst\"\n",
    "city = \"Aarhus C\"\n",
    "postal = \"8000\"\n",
    "country = \"Denmark\" \n",
    "street = \"Ryesgade 1\" # Random location in Aarhus C\n",
    "num_jobs = 50 # Number of jobs that each scraper should fetch at most\n",
    "km_dist = 70 # Search radius in kilometers (Jobnet and JobIndex only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91df351",
   "metadata": {},
   "source": [
    "# Scrape new job listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f140500",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_df = linkedin_scraper(title, city, num_jobs)\n",
    "jobnet_df = jobnet_scraper(title, city, postal, km_dist, num_jobs)\n",
    "jobindex_df = jobindex_scraper(title, city, postal, street, km_dist, num_jobs)\n",
    "\n",
    "# Merge all new job listings\n",
    "df = pd.concat([linkedin_df, jobnet_df, jobindex_df], ignore_index=True)\n",
    "df[\"applied_date\"] = None\n",
    "df[\"reply\"] = None\n",
    "df[\"cover_letter\"] = None\n",
    "df[\"decision\"] = None\n",
    "df[\"decision_reason\"] = None\n",
    "df[\"last_updated\"] = None\n",
    "df[\"cover_letter\"] = None\n",
    "df[\"cv\"] = None\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9625bf",
   "metadata": {},
   "source": [
    "# Filter the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you might apply custom filtering, if the scrapers have fetched too many irrelevant jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aa3e88",
   "metadata": {},
   "source": [
    "# Merge with existing jobs.csv and deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70bf9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"jobs.csv\"):\n",
    "    job_df = pd.DataFrame(columns=df.columns)\n",
    "    job_df.to_csv(\"jobs.csv\", index=False) \n",
    "job_df = pd.read_csv(\"jobs.csv\")\n",
    "\n",
    "df = pd.concat([df, job_df], ignore_index=True)\n",
    "len_before = len(df)\n",
    "df_dups = df[df.duplicated(subset=['company', 'title'], keep=False)].sort_values(by=['company', 'title'])\n",
    "# when dropping duplicates, some of the information might be lost, e.g. if one row has a non-null description but the other has null\n",
    "# we want to keep the non-null description\n",
    "# so we can use groupby with agg to keep the first non-null value for each column\n",
    "dedup_keys = ['company', 'title']\n",
    "agg_funcs = {col: 'first' for col in df.columns if col not in dedup_keys}\n",
    "if 'cover_letter' in df.columns:\n",
    "    agg_funcs['cover_letter'] = lambda s: s.dropna().iloc[0] if s.notna().any() else None\n",
    "if dedup_keys and agg_funcs:\n",
    "    df = (\n",
    "        df.groupby(dedup_keys, as_index=False)\n",
    "        .agg(agg_funcs))\n",
    "elif dedup_keys: # fallback if no agg_funcs defined\n",
    "    df = df.groupby(dedup_keys, as_index=False).first()\n",
    "len_after = len(df)\n",
    "if len_before != len_after:\n",
    "    print(f\"Removed {len_before-len_after} duplicate rows, {len_before} -> {len_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out any duplicates that were found to double check that they were handled correctly\n",
    "df_dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the resulting rows that will be kept from the duplicates\n",
    "df_dups_new = []\n",
    "if len(df_dups) > 0:\n",
    "    df_dups_list = df_dups[['company', 'title']].drop_duplicates().values.tolist()\n",
    "    for company, title in df_dups_list:\n",
    "        df_dups_new.append(df[(df['company'] == company) & (df['title'] == title)])\n",
    "pd.concat(df_dups_new, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated jobs.csv\n",
    "df.to_csv(\"jobs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
